---
title: 네이버 주식토론방 크롤링
date: 2023-11-18 21:22:15 +09:00
categories: [크롤링, selenium, 크롤링, 성공]
tags: [selenium, crawling]
pin: true
---

>대학 자연어처리 과제 준비중!<br/>
>데이터 준비중

# 코드
## version 1
selenium 으로 크롤링을 하기 위해 처음에는 Class name으로 요소를 가저오려 했으나 날짜, 추천, 비추천등 다른 쓸모 없는 요소까지 같은 이름으로 되어있어 다른 방법을 찾아보다 CSS selector를 이용해서 요소를 추출하기로 했다.

<details><summary>코드 version 1</summary>
<div markdown = "1">

```python
import selenium
from selenium import webdriver
import time
import datetime
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC

start = time.time()

def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

page = 2#int(max_page())#최대 99168


all_comment_url = []

date_text = []
title_text = []
body_text = []

for t in range (1, page + 1):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless')
    driver = webdriver.Chrome(options=chrome_options)
    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"
    driver.get(url = url)
    driver.implicitly_wait(10)

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"
    links = driver.find_elements(By.CSS_SELECTOR, selector)
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()
    for url in all_comment_url:
        print(url)
        driver.get(url=url) 
        driver.implicitly_wait(10)
        date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
        title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
        body_not_clean = driver.find_element(By.ID, 'body').text

        body = body_not_clean.replace("\n","")
        title = title_not_clean.replace("#","")
        date_text.append(date)
        title_text.append(title)
        body_text.append(body)
    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t} : ",result[0])

driver.quit()

df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv("data/test.txt", index = False, sep='\t')
df.to_excel("data/test.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])
```

</div>
</details>
<br/>

## version 2
네이버 주식토론방 크롤링을 하다보면 관리자가 삭제한 글이라는 알람이 뜨며 프로그램을 강제종료 시키기에 exception을 걸어 무시하고 지나갈 수 있게 하였다. 코드에 대한 설명은 추후에 천천히 하나씩 채워나가기로 하겠다.

<details><summary>코드 version 2</summary>
<div markdown = "1">


```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By

start = time.time()

def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

page = 10#int(max_page())#최대 99168


all_comment_url = []

date_text = []
title_text = []
body_text = []

for t in range (1, page + 1):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless')
    driver = webdriver.Chrome(options=chrome_options)
    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"
    driver.get(url = url)
    driver.implicitly_wait(10)

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"
    links = driver.find_elements(By.CSS_SELECTOR, selector)
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()
    for url in all_comment_url:
        print(url)
        try:
            driver.get(url=url) 
            driver.implicitly_wait(10)
            date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
            title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
            body_not_clean = driver.find_element(By.ID, 'body').text
            body = body_not_clean.replace("\n","")
            title = title_not_clean.replace("#","")
            date_text.append(date)
            title_text.append(title)
            body_text.append(body)           

        except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
            print("\n관리자가 삭제한 글 임\n")

    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t} : ",result[0])#시간 표시

driver.quit()

#데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv("data/1-100.txt", index = False, sep='\t')
df.to_excel("data/1-100.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

```

</div>
</details>

## version 3
컴퓨터로 코드를 돌려보던 중 이상하게 점점 한 페이지를 크롤링 하는데 걸리는 시간이 늘어나기에 이상함을 느끼고 코드를 다시 보니 전에 크롤링 한 url이 사라지지 않는 문제를 발견하여 다시 코드를 짜 지금은 페이지당 7초로 고정되었음.

크롤링 로그를 저장하기 위해 log.txt라는 파일에 저장하였음.

혹시모를 코드의 또다른 오류로 그동안 크롤링 한 것이 증발하지 않게 10, 100 페이지 마다 저장할 수 있도록 변경 하였음.

설명은 추후 최종 결정하고 기록 예정. (2023-11-18 23:19)

역시 직접 실행해 보고 문제점을 꼭 찾아야 한다는 것을 배웠다... 이걸 모르고 사용했다면.. 으..

<details><summary>코드 version 3</summary>
<div markdown = "1">

```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By
import os.path

#페이지 최대값 자동 불러오기, 굳이 필요 없음
def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

#------------------------------------------------------------------------

file = 'log.txt'
if os.path.isfile(file):
    print("log 파일 존재")
else:
    f = open("log.txt", 'w')
    f.close()


page = 99000#int(max_page())#최대 99168

file = open("log.txt", "a", encoding="UTF-8")
file.write(f"크롤링 페이지 수 : {page}\n\n")
file.close()

start = time.time()
start_middle = time.time()

#중간 저장 리스트 목록
all_comment_url = []
url_100 = []
url_10 = []

date_text_for_100 = []
title_text_for_100 = []
body_text_for_100 = []

date_text_for_10 = []
title_text_for_10 = []
body_text_for_10 = []

date_text = []
title_text = []
body_text = []

#에러(관리자 삭제)수 체크 필수임
error_count = 0

#시간 리셋
tic_10 = 0
tic_100 = 0

#크롤링 시작
for t in range (1, page + 1):
    #크롬 옵션 설정
    chrome_options = webdriver.ChromeOptions()#옵션 드라이버 설정
    chrome_options.add_argument('headless')#웹 드라이버 안띄우고 실행
    chrome_options.add_argument('--no-sandbox')#이걸로 오류를 잡았는데 무엇인지 이해 불가 	Disables the sandbox for all process types that are normally sandboxed. Meant to be used as a browser-level switch for testing purposes only
    driver = webdriver.Chrome(options=chrome_options)#옵션 적용

    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"#comment 크롤링용 링크
    driver.get(url = url)#링크 접속
    driver.implicitly_wait(10)#열릴 때 까지 대기

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"#크롤링 할 url의 CSS selector위치
    links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관
    
    #href값 즉 실질적으로 우리가 필요한 url text를 추출
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()

    for url in all_comment_url:
        #print(url)
        try:
            driver.get(url=url) 
            driver.implicitly_wait(10)
            date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
            title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
            body_not_clean = driver.find_element(By.ID, 'body').text
            body = body_not_clean.replace("\n","")
            title = title_not_clean.replace("#","")
            #append
            date_text_for_100.append(date)
            date_text_for_10.append(date)
            date_text.append(date)
            title_text_for_100.append(title)
            title_text_for_10.append(title)
            title_text.append(title)
            body_text_for_100.append(body)
            body_text_for_10.append(body)
            body_text.append(body)         
            url_100.append(url)
            url_10.append(url)  
        
        except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
            #print("\n관리자가 삭제한 글 임\n")
            error_count = error_count + 1
    all_comment_url = []#리셋 해줘야함 미친미친

    #10페이지 마다 저장(혹시 모를 오류 발생으로 인한 데이터 소실 해소)
    if(t%10 == 0):
        tic_10 = tic_10 + 10
        toc_10 = tic_10 - 10
        df_for_10 = pd.DataFrame()
        df_for_10['date'] = date_text_for_10
        df_for_10['title'] = title_text_for_10
        df_for_10['body'] = body_text_for_10
        df_for_10['url'] = url_10

        date_text_for_10.clear()
        title_text_for_10.clear()
        body_text_for_10.clear()
        url_10.clear()

        df_for_10.to_csv(f"data/for10/{toc_10}_{tic_10}.txt", index = False, sep='\t')
        df_for_10.to_excel(f"data/for10/{toc_10}_{tic_10}.xlsx")

    if(t%100 == 0):
        tic_100 = tic_100 + 100
        toc_100 = tic_100 - 100
        df_for_100 = pd.DataFrame()
        df_for_100['date'] = date_text_for_100
        df_for_100['title'] = title_text_for_100
        df_for_100['body'] = body_text_for_100
        df_for_100['url'] = url_100
        
        date_text_for_100.clear()
        title_text_for_100.clear()
        body_text_for_100.clear()
        url_100.clear()

        df_for_10.to_csv(f"data/for10/{toc_100}_{tic_100}.txt", index = False, sep='\t')
        df_for_10.to_excel(f"data/for10/{toc_100}_{tic_100}.xlsx")


    end_middle = time.time()
    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    sec_middle = (end_middle - start_middle)
    result_middle = str(datetime.timedelta(seconds=sec_middle)).split(".")
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t}시간 : ",result[0])#시간 표시
    print(f"현재까지 시간 : ", result_middle[0])#지금까지 시간 표시
    print(f"관리자 삭제 글 수 : {error_count}")#관리자 삭제 글 수 
    today = time
    file = open("log.txt", "a", encoding="UTF-8")
    text = f"시간 : {today.strftime('%Y-%m-%d %H:%M:%S')}\n페이지{t}의 걸린시간 : {result[0]}\n현재까지 걸린 시간 : {result_middle[0]}\n관리자가 삭제한(error)수 : {error_count}\n\n"
    file.write(text)
    file.close()
driver.quit()

#전체 데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv(f"data/0_{page}_result.txt", index = False, sep='\t')
df.to_excel(f"data/0_{page}_result.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

file = open("log.txt", "a", encoding="UTF-8")
file.write("--------------------------------------------------------\n\n\n")
file.close()
#2023-11-18 미친미친 어쩐지 자꾸 중첩되더라, url이 자꾸 점점 늘어서 그럼, 20페이지 40분에서 4분으로 감소, 35페이지 7분, 한 페이지당 7초
```

</div>
</details>

<br/>

<details><summary>log.txt</summary>
<div markdown = "1">

```txt
크롤링 페이지 수 : 3

시간 : 2023-11-18 23:11:22
페이지1의 걸린시간 : 0:00:08
현재까지 걸린 시간 : 0:00:11
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:11:35
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:24
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:11:48
페이지3의 걸린시간 : 0:00:08
현재까지 걸린 시간 : 0:00:37
관리자가 삭제한(error)수 : 0

--------------------------------------------------------


크롤링 페이지 수 : 3

시간 : 2023-11-18 23:13:03
페이지1의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:11
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:13:14
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:23
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:13:27
페이지3의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:36
관리자가 삭제한(error)수 : 0

--------------------------------------------------------


크롤링 페이지 수 : 99000

시간 : 2023-11-18 23:15:32
페이지1의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:10
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:15:45
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:23
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:15:58
페이지3의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:36
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:11
페이지4의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:49
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:23
페이지5의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:01
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:36
페이지6의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:14
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:49
페이지7의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:26
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:01
페이지8의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:39
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:14
페이지9의 걸린시간 : 0:00:06
현재까지 걸린 시간 : 0:01:51
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:27
페이지10의 걸린시간 : 0:00:08
현재까지 걸린 시간 : 0:02:05
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:40
페이지11의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:02:17
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:51
페이지12의 걸린시간 : 0:00:06
현재까지 걸린 시간 : 0:02:28
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:18:03
페이지13의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:02:41
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:18:16
페이지14의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:02:54
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:18:29
페이지15의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:03:06
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:18:42
페이지16의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:03:19
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:18:54
페이지17의 걸린시간 : 0:00:06
현재까지 걸린 시간 : 0:03:32
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:19:06
페이지18의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:03:44
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:19:19
페이지19의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:03:57
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:19:32
페이지20의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:04:10
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:19:45
페이지21의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:04:22
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:19:57
페이지22의 걸린시간 : 0:00:06
현재까지 걸린 시간 : 0:04:35
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:20:10
페이지23의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:04:47
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:20:23
페이지24의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:05:00
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:20:35
페이지25의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:05:13
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:20:48
페이지26의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:05:26
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:21:01
페이지27의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:05:38
관리자가 삭제한(error)수 : 0


```

</div>
</details>

그리고 다시한번 오류 발생, 이번에는 저번과 다르게 전체 url을 가져오는 과정에서 문제가 생긴 듯 하다. 바로 내일 ver4가 만들어지겠군...
<details><summary>그리고 또다른 오류...</summary>
<div markdown = "1">

```txt
Traceback (most recent call last):
  File "d:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\crawler.py", line 82, in <module>
    links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관  
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 771, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []     
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 228, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: 잘못된 접근입니다.
Message: unexpected alert open: {Alert text : 잘못된 접근입니다.}
```
</div>