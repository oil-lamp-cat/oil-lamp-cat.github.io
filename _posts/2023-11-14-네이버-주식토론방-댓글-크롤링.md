---
title: 네이버 주식토론방 크롤링
date: 2023-11-18 21:22:15 +09:00
categories: [크롤링, selenium, 크롤링, 성공]
tags: [selenium, crawling]
pin: true
---

>대학 자연어처리 과제 준비중!<br/>
>데이터 준비중

# 코드
## version 1
selenium 으로 크롤링을 하기 위해 처음에는 Class name으로 요소를 가저오려 했으나 날짜, 추천, 비추천등 다른 쓸모 없는 요소까지 같은 이름으로 되어있어 다른 방법을 찾아보다 CSS selector를 이용해서 요소를 추출하기로 했다.

<details><summary>코드 version 1</summary>
<div markdown = "1">

```python
import selenium
from selenium import webdriver
import time
import datetime
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC

start = time.time()

def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

page = 2#int(max_page())#최대 99168


all_comment_url = []

date_text = []
title_text = []
body_text = []

for t in range (1, page + 1):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless')
    driver = webdriver.Chrome(options=chrome_options)
    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"
    driver.get(url = url)
    driver.implicitly_wait(10)

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"
    links = driver.find_elements(By.CSS_SELECTOR, selector)
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()
    for url in all_comment_url:
        print(url)
        driver.get(url=url) 
        driver.implicitly_wait(10)
        date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
        title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
        body_not_clean = driver.find_element(By.ID, 'body').text

        body = body_not_clean.replace("\n","")
        title = title_not_clean.replace("#","")
        date_text.append(date)
        title_text.append(title)
        body_text.append(body)
    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t} : ",result[0])

driver.quit()

df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv("data/test.txt", index = False, sep='\t')
df.to_excel("data/test.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])
```

</div>
</details>
<br/>

## version 2
네이버 주식토론방 크롤링을 하다보면 관리자가 삭제한 글이라는 알람이 뜨며 프로그램을 강제종료 시키기에 exception을 걸어 무시하고 지나갈 수 있게 하였다. 코드에 대한 설명은 추후에 천천히 하나씩 채워나가기로 하겠다.

<details><summary>코드 version 2</summary>
<div markdown = "1">


```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By

start = time.time()

def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

page = 10#int(max_page())#최대 99168


all_comment_url = []

date_text = []
title_text = []
body_text = []

for t in range (1, page + 1):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless')
    driver = webdriver.Chrome(options=chrome_options)
    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"
    driver.get(url = url)
    driver.implicitly_wait(10)

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"
    links = driver.find_elements(By.CSS_SELECTOR, selector)
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()
    for url in all_comment_url:
        print(url)
        try:
            driver.get(url=url) 
            driver.implicitly_wait(10)
            date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
            title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
            body_not_clean = driver.find_element(By.ID, 'body').text
            body = body_not_clean.replace("\n","")
            title = title_not_clean.replace("#","")
            date_text.append(date)
            title_text.append(title)
            body_text.append(body)           

        except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
            print("\n관리자가 삭제한 글 임\n")

    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t} : ",result[0])#시간 표시

driver.quit()

#데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv("data/1-100.txt", index = False, sep='\t')
df.to_excel("data/1-100.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

```

</div>
</details>

## version 3
컴퓨터로 코드를 돌려보던 중 이상하게 점점 한 페이지를 크롤링 하는데 걸리는 시간이 늘어나기에 이상함을 느끼고 코드를 다시 보니 전에 크롤링 한 url이 사라지지 않는 문제를 발견하여 다시 코드를 짜 지금은 페이지당 7초로 고정되었음.

크롤링 로그를 저장하기 위해 log.txt라는 파일에 저장하였음.

혹시모를 코드의 또다른 오류로 그동안 크롤링 한 것이 증발하지 않게 10, 100 페이지 마다 저장할 수 있도록 변경 하였음.

설명은 추후 최종 결정하고 기록 예정. (2023-11-18 23:19)

역시 직접 실행해 보고 문제점을 꼭 찾아야 한다는 것을 배웠다... 이걸 모르고 사용했다면.. 으..

<details><summary>코드 version 3</summary>
<div markdown = "1">

```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By
import os.path

#페이지 최대값 자동 불러오기, 굳이 필요 없음
def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

#------------------------------------------------------------------------

file = 'log.txt'
if os.path.isfile(file):
    print("log 파일 존재")
else:
    f = open("log.txt", 'w')
    f.close()


page = 99000#int(max_page())#최대 99168

file = open("log.txt", "a", encoding="UTF-8")
file.write(f"크롤링 페이지 수 : {page}\n\n")
file.close()

start = time.time()
start_middle = time.time()

#중간 저장 리스트 목록
all_comment_url = []
url_100 = []
url_10 = []

date_text_for_100 = []
title_text_for_100 = []
body_text_for_100 = []

date_text_for_10 = []
title_text_for_10 = []
body_text_for_10 = []

date_text = []
title_text = []
body_text = []

#에러(관리자 삭제)수 체크 필수임
error_count = 0

#시간 리셋
tic_10 = 0
tic_100 = 0

#크롤링 시작
for t in range (1, page + 1):
    #크롬 옵션 설정
    chrome_options = webdriver.ChromeOptions()#옵션 드라이버 설정
    chrome_options.add_argument('headless')#웹 드라이버 안띄우고 실행
    chrome_options.add_argument('--no-sandbox')#이걸로 오류를 잡았는데 무엇인지 이해 불가 	Disables the sandbox for all process types that are normally sandboxed. Meant to be used as a browser-level switch for testing purposes only
    driver = webdriver.Chrome(options=chrome_options)#옵션 적용

    print("크롤링 페이지 : ",t)
    url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"#comment 크롤링용 링크
    driver.get(url = url)#링크 접속
    driver.implicitly_wait(10)#열릴 때 까지 대기

    selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"#크롤링 할 url의 CSS selector위치
    links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관
    
    #href값 즉 실질적으로 우리가 필요한 url text를 추출
    for link in links:
        url = link.get_attribute("href")
        all_comment_url.append(url)
    
    time_page_start = time.time()

    for url in all_comment_url:
        #print(url)
        try:
            driver.get(url=url) 
            driver.implicitly_wait(10)
            date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
            title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
            body_not_clean = driver.find_element(By.ID, 'body').text
            body = body_not_clean.replace("\n","")
            title = title_not_clean.replace("#","")
            #append
            date_text_for_100.append(date)
            date_text_for_10.append(date)
            date_text.append(date)
            title_text_for_100.append(title)
            title_text_for_10.append(title)
            title_text.append(title)
            body_text_for_100.append(body)
            body_text_for_10.append(body)
            body_text.append(body)         
            url_100.append(url)
            url_10.append(url)  
        
        except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
            #print("\n관리자가 삭제한 글 임\n")
            error_count = error_count + 1
    all_comment_url = []#리셋 해줘야함 미친미친

    #10페이지 마다 저장(혹시 모를 오류 발생으로 인한 데이터 소실 해소)
    if(t%10 == 0):
        tic_10 = tic_10 + 10
        toc_10 = tic_10 - 10
        df_for_10 = pd.DataFrame()
        df_for_10['date'] = date_text_for_10
        df_for_10['title'] = title_text_for_10
        df_for_10['body'] = body_text_for_10
        df_for_10['url'] = url_10

        date_text_for_10.clear()
        title_text_for_10.clear()
        body_text_for_10.clear()
        url_10.clear()

        df_for_10.to_csv(f"data/for10/{toc_10}_{tic_10}.txt", index = False, sep='\t')
        df_for_10.to_excel(f"data/for10/{toc_10}_{tic_10}.xlsx")

    if(t%100 == 0):
        tic_100 = tic_100 + 100
        toc_100 = tic_100 - 100
        df_for_100 = pd.DataFrame()
        df_for_100['date'] = date_text_for_100
        df_for_100['title'] = title_text_for_100
        df_for_100['body'] = body_text_for_100
        df_for_100['url'] = url_100
        
        date_text_for_100.clear()
        title_text_for_100.clear()
        body_text_for_100.clear()
        url_100.clear()

        df_for_10.to_csv(f"data/for10/{toc_100}_{tic_100}.txt", index = False, sep='\t')
        df_for_10.to_excel(f"data/for10/{toc_100}_{tic_100}.xlsx")


    end_middle = time.time()
    time_end_page = time.time()
    sec = (time_end_page - time_page_start)
    sec_middle = (end_middle - start_middle)
    result_middle = str(datetime.timedelta(seconds=sec_middle)).split(".")
    result = str(datetime.timedelta(seconds=sec)).split(".")
    print(f"페이지{t}시간 : ",result[0])#시간 표시
    print(f"현재까지 시간 : ", result_middle[0])#지금까지 시간 표시
    print(f"관리자 삭제 글 수 : {error_count}")#관리자 삭제 글 수 
    today = time
    file = open("log.txt", "a", encoding="UTF-8")
    text = f"시간 : {today.strftime('%Y-%m-%d %H:%M:%S')}\n페이지{t}의 걸린시간 : {result[0]}\n현재까지 걸린 시간 : {result_middle[0]}\n관리자가 삭제한(error)수 : {error_count}\n\n"
    file.write(text)
    file.close()
driver.quit()

#전체 데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv(f"data/0_{page}_result.txt", index = False, sep='\t')
df.to_excel(f"data/0_{page}_result.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

file = open("log.txt", "a", encoding="UTF-8")
file.write("--------------------------------------------------------\n\n\n")
file.close()
#2023-11-18 미친미친 어쩐지 자꾸 중첩되더라, url이 자꾸 점점 늘어서 그럼, 20페이지 40분에서 4분으로 감소, 35페이지 7분, 한 페이지당 7초
```

</div>
</details>

<br/>

<details><summary>log.txt</summary>
<div markdown = "1">

```txt
크롤링 페이지 수 : 3

시간 : 2023-11-18 23:11:22
페이지1의 걸린시간 : 0:00:08
현재까지 걸린 시간 : 0:00:11
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:11:35
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:24
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:11:48
페이지3의 걸린시간 : 0:00:08
현재까지 걸린 시간 : 0:00:37
관리자가 삭제한(error)수 : 0

--------------------------------------------------------


크롤링 페이지 수 : 3

시간 : 2023-11-18 23:13:03
페이지1의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:11
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:13:14
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:23
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:13:27
페이지3의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:36
관리자가 삭제한(error)수 : 0

--------------------------------------------------------


크롤링 페이지 수 : 99000

시간 : 2023-11-18 23:15:32
페이지1의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:10
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:15:45
페이지2의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:23
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:15:58
페이지3의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:36
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:11
페이지4의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:00:49
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:23
페이지5의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:01
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:36
페이지6의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:14
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:16:49
페이지7의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:26
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:01
페이지8의 걸린시간 : 0:00:07
현재까지 걸린 시간 : 0:01:39
관리자가 삭제한(error)수 : 0

시간 : 2023-11-18 23:17:14
페이지9의 걸린시간 : 0:00:06
현재까지 걸린 시간 : 0:01:51
관리자가 삭제한(error)수 : 0

```

</div>
</details>

그리고 다시한번 오류 발생, 이번에는 저번과 다르게 전체 url을 가져오는 과정에서 문제가 생긴 듯 하다. 바로 내일 ver4가 만들어지겠군...
<details><summary>그리고 또다른 오류...</summary>
<div markdown = "1">

```txt
Traceback (most recent call last):
  File "d:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\crawler.py", line 82, in <module>
    links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관  
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 771, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []     
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "D:\AFTER 2021-11-21\programing\NLP_TERM_PROJECT\NLP_TP\Lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 228, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: 잘못된 접근입니다.
Message: unexpected alert open: {Alert text : 잘못된 접근입니다.}
```
</div>
</details>

## version 4
이번에 찾은 문제점은 저번 것에 이어서 추가적으로 
```python
openpyxl.utils.exceptions.IllegalCharacterError
```
이라는 오류를 만나게 되었고 이를 해결하기 위해
```python
from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
```
을 이용해 특수문자를 제거하는 작업을 추가해 주었다.

<details><summary>ver4</summary>
<div markdown = "1">

변경사항 : 크롤링용 url을 로딩하던 도중 오류 발생, 특수문자 exel파일로 옮기기 불가, 파일 로그 오류 url추가

```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By
import os.path
from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE

#페이지 최대값 자동 불러오기, 굳이 필요 없음
def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

#------------------------------------------------------------------------

file = 'log.txt'
if os.path.isfile(file):
    print("log 파일 존재")
else:
    f = open("log.txt", 'w')
    f.close()

start_page = 1#0페이지 존재 안함 0넣으면 문제가 좀 많이많이 커져용
page = 99000#int(max_page())#최대 99168

file = open("log.txt", "a", encoding="UTF-8")
file.write(f"\n-------------------------------\n크롤링 시작 페이지 수: {start_page}\n크롤링 총 페이지 수 : {page}\n\n")
file.close()

start = time.time()
start_middle = time.time()

#중간 저장 리스트 목록
all_comment_url = []
url_100 = []
url_10 = []

date_text_for_100 = []
title_text_for_100 = []
body_text_for_100 = []

date_text_for_10 = []
title_text_for_10 = []
body_text_for_10 = []

date_text = []
title_text = []
body_text = []

#에러(관리자 삭제)수 체크 필수임
error_count = 0

#시간 리셋
tic_10 = 0
tic_100 = 0

#크롤링 시작
for t in range (start_page, page + 1):
    try:
        #크롬 옵션 설정
        chrome_options = webdriver.ChromeOptions()#옵션 드라이버 설정
        chrome_options.add_argument('headless')#웹 드라이버 안띄우고 실행
        chrome_options.add_argument('--no-sandbox')#이걸로 오류를 잡았는데 무엇인지 이해 불가 	Disables the sandbox for all process types that are normally sandboxed. Meant to be used as a browser-level switch for testing purposes only
        chrome_options.add_argument('disable-gpu')#gpu사용 안함
        driver = webdriver.Chrome(options=chrome_options)#옵션 적용

        print("크롤링 페이지 : ",t)
        url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"#comment 크롤링용 링크
        driver.get(url = url)#링크 접속
        driver.implicitly_wait(10)#열릴 때 까지 대기

        selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"#크롤링 할 url의 CSS selector위치
        links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관
        
        #href값 즉 실질적으로 우리가 필요한 url text를 추출
        for link in links:
            url = link.get_attribute("href")
            all_comment_url.append(url)
        
        time_page_start = time.time()

        for url in all_comment_url:
            #print(url)
            try:
                driver.get(url=url) 
                driver.implicitly_wait(10)
                date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
                title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
                body_not_clean = driver.find_element(By.ID, 'body').text
                body = body_not_clean.replace("\n","")
                title = title_not_clean.replace("#","")
                #append
                date_text_for_100.append(date)
                date_text_for_10.append(date)
                date_text.append(date)
                title_text_for_100.append(title)
                title_text_for_10.append(title)
                title_text.append(title)
                body_text_for_100.append(body)
                body_text_for_10.append(body)
                body_text.append(body)         
                url_100.append(url)
                url_10.append(url)  
            
            except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
                #print("\n관리자가 삭제한 글 임\n")
                error_count = error_count + 1
        all_comment_url = []#리셋 해줘야함 미친미친

        #10페이지 마다 저장(혹시 모를 오류 발생으로 인한 데이터 소실 해소)
        if(t%10 == 0):
            tic_10 = tic_10 + 10
            toc_10 = tic_10 - 10
            df_for_10 = pd.DataFrame()
            df_for_10['date'] = date_text_for_10
            df_for_10['title'] = title_text_for_10
            df_for_10['body'] = body_text_for_10
            df_for_10['url'] = url_10

            date_text_for_10.clear()
            title_text_for_10.clear()
            body_text_for_10.clear()
            url_10.clear()

            df_for_10.to_csv(f"data/for10/{toc_10}_{tic_10}.txt", index = False, sep='\t')
            df_for_10.to_excel(f"data/for10/{toc_10}_{tic_10}.xlsx")
            file = open("log.txt", "a", encoding="UTF-8")
            txt_text = f"{t}파일 생성 : {toc_10}_{tic_10}.txt"
            file.write(txt_text)
            file.close()


        if(t%100 == 0):
            tic_100 = tic_100 + 100
            toc_100 = tic_100 - 100
            df_for_100 = pd.DataFrame()
            df_for_100['date'] = date_text_for_100
            df_for_100['title'] = title_text_for_100
            df_for_100['body'] = body_text_for_100
            df_for_100['url'] = url_100
            
            date_text_for_100.clear()
            title_text_for_100.clear()
            body_text_for_100.clear()
            url_100.clear()

            df_for_10.to_csv(f"data/for10/{toc_100}_{tic_100}.txt", index = False, sep='\t')
            df_for_10.to_excel(f"data/for10/{toc_100}_{tic_100}.xlsx")
            file = open("log.txt", "a", encoding="UTF-8")
            txt_text = f"{t}파일 생성 : {toc_100}_{tic_100}.txt"
            file.write(txt_text)
            file.close()


        end_middle = time.time()
        time_end_page = time.time()
        sec = (time_end_page - time_page_start)
        sec_middle = (end_middle - start_middle)
        result_middle = str(datetime.timedelta(seconds=sec_middle)).split(".")
        result = str(datetime.timedelta(seconds=sec)).split(".")
        print(f"페이지{t}시간 : ",result[0])#시간 표시
        print(f"현재까지 시간 : ", result_middle[0])#지금까지 시간 표시
        print(f"관리자 삭제 글 수 : {error_count}")#관리자 삭제 글 수 
        today = time
        file = open("log.txt", "a", encoding="UTF-8")
        text = f"시간 : {today.strftime('%Y-%m-%d %H:%M:%S')}\n페이지{t}의 걸린시간 : {result[0]}\n현재까지 걸린 시간 : {result_middle[0]}\n관리자가 삭제한(error)수 : {error_count}\n\n"
        file.write(text)
        file.close()
    except UnexpectedAlertPresentException:
        print(url)
        print("크롤링용 url 오류 발생지")
        file = open("log.txt", "a", encoding="UTF-8")
        error_text = f"크롤링용 url 오류 발생 {url}"
        file.write(error_text)
        file.close()

driver.quit()

#전체 데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv(f"data/0_{page}_result.txt", index = False, sep='\t')
page = ILLEGAL_CHARACTERS_RE.sub(r'', text)#엑셀파일 불가한 특수문자 제거 -> 7초에서 11초 정도로 시간 증가
df.to_excel(f"data/0_{page}_result.xlsx")
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

file = open("log.txt", "a", encoding="UTF-8")
file.write("--------------------------------------------------------\n\n\n")
file.close()
#2023-11-18 미친미친 어쩐지 자꾸 중첩되더라, url이 자꾸 점점 늘어서 그럼, 20페이지 40분에서 4분으로 감소, 35페이지 7분, 한 페이지당 7초
#2023-11-21 였는데 특수문자 처리 하는 속도가 늘어나서 페이지당 10초로 증가
```

</div>
</details>
<br/>
에서 결국 바로 또 다른 오류 발생

<details><summary>오류</summary>
<div markdown = "1">

```cmd
DevTools listening on ws://127.0.0.1:2469/devtools/browser/0428c71e-8a66-41c2-abcd-fa36b4ebde1a
크롤링 페이지 :  20
Traceback (most recent call last):
  File "d:\CODE\NLP_TERM_PROJECT\crawler.py", line 138, in <module>
    df_for_10.to_excel(f"data/for10/{toc_10}_{tic_10}.xlsx") 
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\pandas\core\generic.py", line 2345, in to_excel  
    formatter.write(
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\pandas\io\formats\excel.py", line 955, in write  
    writer._write_cells(
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\pandas\io\excel\_openpyxl.py", line 490, in _write_cells
    xcell.value, fmt = self._value_with_fmt(cell.val)
    ^^^^^^^^^^^
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\openpyxl\cell\cell.py", line 218, in value       
    self._bind_value(value)
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\openpyxl\cell\cell.py", line 197, in _bind_value 
    value = self.check_string(value)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CODE\NLP_TERM_PROJECT\NLP_TERM_PROJECT_VENV\Lib\site-packages\openpyxl\cell\cell.py", line 165, in check_string
    raise IllegalCharacterError(f"{value} cannot be used in worksheets.")
openpyxl.utils.exceptions.IllegalCharacterError: => 윤대통령, 영국.프랑스 방문 타이밍이 기가 막히게 좋구나==> 사우디 ,이탈리아 ,한국 부산 3파전 1차투표로 2개국 선정 그리고 결승은 2차 투표로1차투표 예상 2개국으로 한국 ,사우디로 , 한국은 이탈리아 투표한 국가들 적극 포섭 전략오는 2030년 세계박람회(엑스포)  개최 도시를 정하는 국제박람회기구(BIE) 총회가 어느덧 1주일여 앞으로 다가왔다. BIE는 오는 28일(현지시간) 프랑스 파리에서 총회를 열고 부산과 사우디아라비아 리야드,이탈인과 국가가 실력을 겨루는 자리라면 엑스포는 한 국가의 산업,과학기술,문화 수준을 과시하는 경제문화올림픽이라 할 수 있다. 엑스포는 전시 기간과 규모 등에 따라 등록박람회와인정박람회로 나뉘는데, 한국은 1993년 대전과2012년 전남 여수에서 인정박람회를 개최한 바 있다.부산이 도전장을 낸 것은 등록박람회로, 한국이 유치에  성공한다면 올림픽, 월드컵등 3대 글로벌 이벤트를 모두개최하는 7번째 국가가 된다.유치위에 따르면 부산엑스포가 열리면 관람객만 3천500만명에 이르고 60조원 이상의생산유발, 50만명의 고용창출 효과를 낳을 것으로 추산된다. 비단 경제적 효과뿐만 아니라 IT와 미디어 등 '스마트 혁신' 강국으로 서의국가 위상을 높이고 한류 등 소 프트파워를 확산시킬 수 있는 계기가 될 것으로 기대된다. cannot be used in worksheets.
```
</div>
</details>
읽어보니 아무래도 excel worksheets에 들어갈 수 있는 최대 글 길이를 넘어선 듯 하다. 좀 더 생각해 보아햐 할 듯한 문제다...

## version 5
그렇다면 문제를 해결하기 위해 아예 excel이 아닌 csv형식을 사용하는 것은 어떤가? 훨씬 빠르고 제약이 없는 csv를 이용해서 문제 해결

<details><summary>ver5</summary>
<div markdown = "1">

```python
import selenium
from selenium import webdriver
import time
import datetime
from selenium.common.exceptions import UnexpectedAlertPresentException
import pandas as pd
from selenium.webdriver.common.by import By
import os.path
#from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE

#페이지 최대값 자동 불러오기, 굳이 필요 없음
def max_page():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('headless') #코드 완벽할 시 활성화
    #mobile_emulation = {"deviceName": "iPhone 12 Pro"}
    #chrome_options.add_experimental_option("mobileEmulation", mobile_emulation)#모바일 모드 용도
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.naver.com/item/board.naver?code=005930")
    driver.find_element(By.CLASS_NAME, 'pgRR').click()
    max_not_clean = driver.find_element(By.CSS_SELECTOR, '#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td.on > a').text
    driver.quit()
    max = max_not_clean.replace(",","")
    print("최대값 : ",max)
    return max

#------------------------------------------------------------------------

file = 'log.txt'
if os.path.isfile(file):
    print("log 파일 존재")
else:
    f = open("log.txt", 'w')
    f.close()

start_page = 1#0페이지 존재 안함 0넣으면 문제가 좀 많이많이 커져용
page = 99000#int(max_page())#최대 99168

file = open("log.txt", "a", encoding="UTF-8")
file.write(f"\n-------------------------------\n크롤링 시작 페이지 수: {start_page}\n크롤링 총 페이지 수 : {page}\n\n")
file.close()

start = time.time()
start_middle = time.time()

#중간 저장 리스트 목록
all_comment_url = []
url_100 = []
url_10 = []

date_text_for_100 = []
title_text_for_100 = []
body_text_for_100 = []

date_text_for_10 = []
title_text_for_10 = []
body_text_for_10 = []

date_text = []
title_text = []
body_text = []

#에러(관리자 삭제)수 체크 필수임
error_count = 0

#시간 리셋
tic_10 = 0
tic_100 = 0

#크롤링 시작
for t in range (start_page, page + 1):
    try:
        #크롬 옵션 설정
        chrome_options = webdriver.ChromeOptions()#옵션 드라이버 설정
        chrome_options.add_argument('headless')#웹 드라이버 안띄우고 실행
        chrome_options.add_argument('--no-sandbox')#이걸로 오류를 잡았는데 무엇인지 이해 불가 	Disables the sandbox for all process types that are normally sandboxed. Meant to be used as a browser-level switch for testing purposes only
        chrome_options.add_argument('disable-gpu')#gpu사용 안함
        driver = webdriver.Chrome(options=chrome_options)#옵션 적용

        print("크롤링 페이지 : ",t)
        url = f"https://finance.naver.com/item/board.naver?code=005930&page={t}"#comment 크롤링용 링크
        driver.get(url = url)#링크 접속
        driver.implicitly_wait(10)#열릴 때 까지 대기

        selector = "#content > div.section.inner_sub > table > tbody > tr > td.title > a"#크롤링 할 url의 CSS selector위치
        links = driver.find_elements(By.CSS_SELECTOR, selector)#요소(link) 찾아서 links에 전부 보관
        
        #href값 즉 실질적으로 우리가 필요한 url text를 추출
        for link in links:
            url = link.get_attribute("href")
            all_comment_url.append(url)
        
        time_page_start = time.time()

        for url in all_comment_url:
            #print(url)
            try:
                driver.get(url=url) 
                driver.implicitly_wait(10)
                date = driver.find_element(By.CLASS_NAME, 'gray03.p9.tah').text
                title_not_clean = driver.find_element(By.CLASS_NAME, 'c.p15').text
                body_not_clean = driver.find_element(By.ID, 'body').text
                body = body_not_clean.replace("\n","")
                title = title_not_clean.replace("#","")
                #append
                date_text_for_100.append(date)
                date_text_for_10.append(date)
                date_text.append(date)
                title_text_for_100.append(title)
                title_text_for_10.append(title)
                title_text.append(title)
                body_text_for_100.append(body)
                body_text_for_10.append(body)
                body_text.append(body)         
                url_100.append(url)
                url_10.append(url)  
            
            except UnexpectedAlertPresentException:#무조건 필요 관리자가 삭제한 글을 불러오지 못할 때에 문제가 아주 크게크게 생김 프로그램이 멈춰버려!
                #print("\n관리자가 삭제한 글 임\n")
                error_count = error_count + 1
        all_comment_url = []#리셋 해줘야함 미친미친

        #10페이지 마다 저장(혹시 모를 오류 발생으로 인한 데이터 소실 해소)
        if(t%10 == 0):
            tic_10 = tic_10 + 10
            toc_10 = tic_10 - 10
            df_for_10 = pd.DataFrame()
            df_for_10['date'] = date_text_for_10
            df_for_10['title'] = title_text_for_10
            df_for_10['body'] = body_text_for_10
            df_for_10['url'] = url_10

            date_text_for_10.clear()
            title_text_for_10.clear()
            body_text_for_10.clear()
            url_10.clear()

            df_for_10.to_csv(f"data/for10/{toc_10}_{tic_10}.txt", index = False, sep='\t')
            df_for_10.to_csv(f"data/for10/{toc_10}_{tic_10}.csv", index=False, sep='\t')
            file = open("log.txt", "a", encoding="UTF-8")
            txt_text = f"{t}파일 생성 : {toc_10}_{tic_10}.txt"
            file.write(txt_text)
            file.close()


        if(t%100 == 0):
            tic_100 = tic_100 + 100
            toc_100 = tic_100 - 100
            df_for_100 = pd.DataFrame()
            df_for_100['date'] = date_text_for_100
            df_for_100['title'] = title_text_for_100
            df_for_100['body'] = body_text_for_100
            df_for_100['url'] = url_100
            
            date_text_for_100.clear()
            title_text_for_100.clear()
            body_text_for_100.clear()
            url_100.clear()

            df_for_10.to_csv(f"data/for10/{toc_100}_{tic_100}.txt", index = False, sep='\t')
            df_for_10.to_csv(f"data/for10/{toc_100}_{tic_100}.csv", index=False, sep='\t')
            file = open("log.txt", "a", encoding="UTF-8")
            txt_text = f"{t}파일 생성 : {toc_100}_{tic_100}.txt"
            file.write(txt_text)
            file.close()


        end_middle = time.time()
        time_end_page = time.time()
        sec = (time_end_page - time_page_start)
        sec_middle = (end_middle - start_middle)
        result_middle = str(datetime.timedelta(seconds=sec_middle)).split(".")
        result = str(datetime.timedelta(seconds=sec)).split(".")
        print(f"페이지{t}시간 : ",result[0])#시간 표시
        print(f"현재까지 시간 : ", result_middle[0])#지금까지 시간 표시
        print(f"관리자 삭제 글 수 : {error_count}")#관리자 삭제 글 수 
        today = time
        file = open("log.txt", "a", encoding="UTF-8")
        text = f"시간 : {today.strftime('%Y-%m-%d %H:%M:%S')}\n페이지{t}의 걸린시간 : {result[0]}\n현재까지 걸린 시간 : {result_middle[0]}\n관리자가 삭제한(error)수 : {error_count}\n\n"
        file.write(text)
        file.close()
    except UnexpectedAlertPresentException:
        print(url)
        print("크롤링용 url 오류 발생지")
        file = open("log.txt", "a", encoding="UTF-8")
        error_text = f"크롤링용 url 오류 발생 {url}"
        file.write(error_text)
        file.close()

driver.quit()

#전체 데이터 프레임 만들어서 저장
df = pd.DataFrame()
df['date'] = date_text
df['title'] = title_text
df['body'] =  body_text

df.to_csv(f"data/0_{page}_result.txt", index = False, sep='\t')
#page = ILLEGAL_CHARACTERS_RE.sub(r'', text)#엑셀파일 불가한 특수문자 제거 -> 7초에서 11초 정도로 시간 증가
df.to_csv(f"data/0_{page}_result.csv", index=False, sep='\t')#엑셀로 포함 불가능한 길이의 문장들이 존재
end = time.time()
sec = (end - start)
result = str(datetime.timedelta(seconds=sec)).split(".")
print("전체 페이지 : ",result[0])

file = open("log.txt", "a", encoding="UTF-8")
file.write("--------------------------------------------------------\n\n\n")
file.close()
#2023-11-18 미친미친 어쩐지 자꾸 중첩되더라, url이 자꾸 점점 늘어서 그럼, 20페이지 40분에서 4분으로 감소, 35페이지 7분, 한 페이지당 7초
#2023-11-21 excel 파일로 만들기에는 문장이 길어서 넣지 못하는 경우가 생겨 csv로 생성
```

</div>
</details>